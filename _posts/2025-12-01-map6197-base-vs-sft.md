---
title: "MAP6197 Final Project: Opening the Box — Base vs. SFT"
author: Spencer Moore
date: 2025-12-01
math: true
categories: [machine learning, LLM]
tags: [LLM, LoRA, SVD, mechanistic interpretability, Qwen]
description: "Investigating how supervised fine-tuning shifts LLM weight matrices using SVD rank analysis on Qwen 2.5 models"
---

## Introduction

This post presents my MAP6197 final project investigating **how supervised fine-tuning (SFT) modifies the internal weight matrices of large language models**. Using the Qwen 2.5 family of models, I compared base (pre-trained) and instruct (fine-tuned) versions to understand what changes at the matrix level when we adapt a model for instruction-following.

![Title Slide](/assets/img/posts/2025-12-01-map6197-sft-01.png)

The central question: **Can we just look at the weight matrices themselves and see for ourselves what SFT does?**

---

## Project Agenda

![Agenda](/assets/img/posts/2025-12-01-map6197-sft-02.png)

The investigation followed five main threads:

1. **What else was attempted** — Initial explorations with data mining
2. **Review of Rank** — Mathematical foundations
3. **Pre-Training & Supervised Fine-Tuning** — Understanding the training paradigm
4. **Methods: Software/Hardware** — Implementation details
5. **Unsurprising Results & Future Questions** — Findings and next steps

---

## What Else Was Attempted: Data Mining Physics Opinions

![Code Screenshot](/assets/img/posts/2025-12-01-map6197-sft-03.png)

Before diving into weight analysis, I attempted to measure differences between base and SFT models using the **Stack Exchange Data Dump** — a ~90 GB torrent containing everything from the site.

The goal was to create a repository of physics posts and measure **perplexity differences** between base and instruction-tuned models on informed vs. less-informed physics content.

---

### Results: Spectral Norm of Weight Differences

![Spectral Norm Results](/assets/img/posts/2025-12-01-map6197-sft-04.png)

At 1.5B parameters, the relative rank metric \( W_\Delta / W_{Base} \) showed potential for LoRA mechanisms. But what do we care about beyond computational convenience?

This cuts to the heart of **mechanistic interpretability**: SFT strengthens some features while weakening others, working with existing structures that matter for the task.

---

### Data Mining Setup

![HuggingFace Qwen Models](/assets/img/posts/2025-12-01-map6197-sft-05.png)

Qwen models weren't the only ones run with open base/SFT pairs. However, they allowed scaling up in parameters to track the effects across model sizes.

---

### Architecture Diagram

![Qwen Architecture](/assets/img/posts/2025-12-01-map6197-sft-06.png)

NOTE: This diagram shows Qwen 3, not Qwen 2.5. But 2.5 (as seen in the code) does have 3 projection matrices for the MLP, not just 2.

Note the use of the **conditional gate** in the MLP, which makes it a gated part. The **K and Q projection matrices** are critical for attention mechanisms that depend on position via Rotary Positional Encoding (RoPE).

---

### Stack Exchange Data Mining

![Stack Exchange Screenshot](/assets/img/posts/2025-12-01-map6197-sft-07.png)

Stack Exchange has ~90 GB openly available with everything on a torrent. I vibe-coded a repository to extract physics posts and ended up with:

- Two groups of 100 prompts each
  - Group of votes -3 or lower
  - Group of votes +4 or higher
- Scrubbed of markdown elements
- At minimum 1000 characters (~200 tokens)

The goal was measuring perplexity (relative surprise) differences between Base and SFT models.

---

## Review: Let's Talk About Rank

![Review of Rank - 3D Visualization](/assets/img/posts/2025-12-01-map6197-sft-08.png)

**Mental exercise** to illustrate what we've already explored mathematically:

The outer product of a column and row (either 1,000 elements long) is itself 1,000,000 elements. No new information is created by this operation.

This is **precisely why matrices have inherent measures describing degrees of freedom**.

The outer product illustration shows a 2D space but the math extends to any dimension:

\[
C = a \otimes b = ab^T = \begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ a_3 \end{bmatrix} \begin{bmatrix} b_0 & b_1 & b_2 & b_3 \end{bmatrix} = \begin{bmatrix} a_0b_0 & a_0b_1 & a_0b_2 & a_0b_3 \\\\ a_1b_0 & a_1b_1 & a_1b_2 & a_1b_3 \\\\ a_2b_0 & a_2b_1 & a_2b_2 & a_2b_3 \\\\ a_3b_0 & a_3b_1 & a_3b_2 & a_3b_3 \end{bmatrix}
\]

---

### Autoencoders and Latent Space

![Autoencoders](/assets/img/posts/2025-12-01-map6197-sft-09.png)

Autoencoders provide another lens on this concept. The **latent space** bottleneck forces compression, and the decoder reconstructs from this reduced representation — analogous to low-rank approximations.

---

### Connection to LoRA

![LoRA Diagram](/assets/img/posts/2025-12-01-map6197-sft-10.png)

We KNOW that:

- We can detect rank with **SVD** (Singular Value Decomposition)
- This puts bounds on information content

We often use **"Low-Rank Adaptation" (LoRA)** to describe Supervised Fine-Tuning (SFT). Can we just look at the weight matrices and see for ourselves?

The LoRA formulation:

\[
W_{SFT} = W_{Base} + W_\Delta = W_{Base} + \tilde{A}^T \otimes \vec{B}
\]

\[
\text{Rank}(\tilde{A}^T \otimes \vec{B}) = \min(\text{Rank}(\tilde{A}^T), \text{Rank}(\vec{B}))
\]

So knowing this technique is used, can we then ask:

\[
\text{Rank}(W_{SFT} - W_{Base}) \stackrel{?}{\lesssim} \text{Rank}(W_{Base})
\]

---

## Base Model vs. Instruct Model

![Base vs Instruct Training](/assets/img/posts/2025-12-01-map6197-sft-11.png)

### Pre-Training: Base Model

**Unsupervised Training**
- NOTE: just given all of human language to try and predict next token and that is all
- Vast majority of computation
- Sets weights from an initial randomized value

### Supervised Fine-Tuning: Instruct Model

**Supervised Training**
- Given model conversations to train to match outputs
- Lots of computation but nothing compared to pretraining
- Shifts weights by no more than 1% at most

---

## Methods: Code

![Python Code](/assets/img/posts/2025-12-01-map6197-sft-12.png)

**LEFT COLUMN:**

Code for running and storing results of calculations of individual weight matrices (lowest level workhorse function).

**RIGHT COLUMN:**

Here we have the basics of vibe-coded running of model pairs with information necessary to get them off HuggingFace.

---

### Methods: Remote Hardware

![Google Colab A100](/assets/img/posts/2025-12-01-map6197-sft-13.png)

**Remote Google Colab Student Tier A100**

This is a reasonably powerful card which is 40 GB and DOES technically allow for loading of layers one at a time in GPU RAM. However, it seems likely that the CPU was called based on the time it took to run the analysis on the weight matrices.

---

## Results: Relative Changes by Layer and Metric

![Relative Changes Heatmap](/assets/img/posts/2025-12-01-map6197-sft-14.png)

Again, the 0.5B is the outlier in terms of dramatic shifts.

Unsurprisingly we see larger shifts when looking at a larger number of parameters crammed into few layers (28 for 7B versus 36 for 3B).

Also, most of the large shifts seem to (again as expected) take place in **q and k projection matrices** (attention position encoding).

---

### Results: Spectral Norm

![Spectral Norm Chart](/assets/img/posts/2025-12-01-map6197-sft-15.png)

Nothing too surprising here. We would expect dramatic differences to start appearing in the intermediate layers and build up near the end.

Also this establishes **0.5B model as an outlier**.

---

### Methods: Software/Hardware — HuggingFace Qwen

![HuggingFace Screenshot](/assets/img/posts/2025-12-01-map6197-sft-16.png)

Qwen models were not the only ones run which have open weight base/SFT pairs. However they allowed scaling up in terms of parameters to track effects.

---

### Methods: Code Details

![Code Implementation](/assets/img/posts/2025-12-01-map6197-sft-17.png)

LEFT COLUMN: Code for running and storing results of calculations of individual weight matrices.

RIGHT COLUMN: Vibe-coded basics for running model pairs with information to get them off HuggingFace.

---

## Results: Relative Rank

![Relative Rank Chart](/assets/img/posts/2025-12-01-map6197-sft-18.png)

At last, we see where (even at 1.5B parameters) the relative rank denotes the potential for LoRA mechanisms.

BUT what do we care about in this description beyond computational convenience in packaging SFT as an extra stage?

This cuts to the heart of **mechanistic interpretability**: SFT strengthens and weakens features, but in some way works with existing FEATURES that matter for SFT (hence the esocin[?] mechanism).

---

### Perplexity Results

![Perplexity Plots](/assets/img/posts/2025-12-01-map6197-sft-19.png)

We actually have a few interesting plots to go over!

This is the **cumulative per-token delta in perplexity**.

- The slope on the left is strictly based on the value of the right
- The value of the right is based on the change in logprob from the previous token

---

### Running Residual Layer Differences

![Residual Code](/assets/img/posts/2025-12-01-map6197-sft-20.png)

Running residual layer-by-layer differences turned out to be challenging to implement properly. The goal was to systematically track how changes propagate through the network.

---

## Mechanistic Interpretability: Features & Circuits

![Features and Circuits](/assets/img/posts/2025-12-01-map6197-sft-22.png)

### Future Questions & Directions

The **replacement model** approach replaces transformer neurons with more interpretable features, while **attribution graphs** depict the influence of features on one another, allowing us to trace intermediate steps the model uses to produce its output.

Key questions remain:

- How do the low-rank updates identified in our analysis map to specific feature changes?
- Can we trace which semantic capabilities are enhanced by the SFT process?
- What circuits are being strengthened vs. weakened during instruction tuning?

This work establishes that SFT does indeed operate via relatively low-rank updates to weight matrices, consistent with LoRA assumptions. The next step is connecting these mathematical observations to **mechanistic understanding** of what the model learns.

---

*This project was completed for MAP6197. Code and additional analysis available upon request.*
